{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4428d812",
   "metadata": {},
   "source": [
    "- Rdkit : 분자를 Graph 형태로 만들고 분자의 logP 특성을 불러올 수 있는 Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91282635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install rdkit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03967761",
   "metadata": {},
   "source": [
    "- 분자를 텍스트 형태로 표현한 smiles와 molecular graph로 표현하기 위한 vocab.npy 파일."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e221e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 5374k  100 5374k    0     0  5394k      0 --:--:-- --:--:-- --:--:-- 5418k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   256  100   256    0     0    504      0 --:--:-- --:--:-- --:--:--   509\n"
     ]
    }
   ],
   "source": [
    "!curl -o ZINC.smiles https://raw.githubusercontent.com/heartcored98/Standalone-DeepLearning/master/Lec9/ZINC.smiles\n",
    "!curl -o vocab.npy https://raw.githubusercontent.com/heartcored98/Standalone-DeepLearning/master/Lec9/vocab.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0de468",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a8a6358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.Crippen import MolLogP\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "746a7450",
   "metadata": {},
   "outputs": [],
   "source": [
    "paser = argparse.ArgumentParser()\n",
    "# Set parser\n",
    "args = paser.parse_args(\"\")\n",
    "args.seed = 123\n",
    "args.val_size = 0.1\n",
    "args.test_size = 0.1\n",
    "args.shuffle = True\n",
    "# Set Parser Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e71501b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e31b59d",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b4e3cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ZINC_smiles(file_name, num_mol):\n",
    "    f = open(file_name, 'r')\n",
    "    contents = f.readlines()\n",
    "    \n",
    "    smi_list = []\n",
    "    logP_list = []\n",
    "    for i in tqdm_notebook(range(num_mol), desc = 'Reading Data'):\n",
    "        smi = contents[i].strip()\n",
    "        # Read One line from contents\n",
    "        m=Chem.MolFromSmiles(smi)\n",
    "        smi_list.append(smi)\n",
    "        logP_list.append(MolLogP(m))\n",
    "        \n",
    "    logP_list=np.asarray(logP_list).astype(float)\n",
    "    \n",
    "    # 1. smi_list : read string\n",
    "    # 2. logP_list : MolFromSmiles and MolLogP\n",
    "    return smi_list, logP_list\n",
    "\n",
    "def smiles_to_onehot(smi_list):\n",
    "    def smiles_to_vector(smiles, vocab, max_length):\n",
    "        while len(smiles) < max_length:\n",
    "            smiles +=\" \"\n",
    "        vector = [vocab.index(str(x)) for x in smiles]\n",
    "        one_hot = np.zeros((len(vocab), max_length),dtype=int)\n",
    "        for i,elm in enumerate(vector):\n",
    "            one_hot[elm][i] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    vocab = np.load('./vocab.npy')\n",
    "    smi_total=[]\n",
    "    \n",
    "    for i,smi in tqdm_notebook(enumerate(smi_list), desc='Converting to One Hot'):\n",
    "        smi_onehot = smiles_to_vector(smi, list(vocab),120)\n",
    "        smi_total.append(smi_onehot)\n",
    "    \n",
    "    return np.asarray(smi_total)\n",
    "\n",
    "def convert_to_graph(smiles_list):\n",
    "    adj=[]\n",
    "    adj_norm=[]\n",
    "    features=[]\n",
    "    maxNumAtoms=50\n",
    "    \n",
    "    for i in tqdm_notebook(smiles_list, desc='Converting to Graph'):\n",
    "        # Mol\n",
    "        iMol = Chem.MolFromSmiles(i.strip())\n",
    "        # Adj\n",
    "        iAdjTmp = Chem.rdmolops.GetAdjacencyMatrix(iMol)\n",
    "        # Feature\n",
    "        if( iAdjTmp.shape[0] <= maxNumAtoms):\n",
    "            #Feature-preprocessing\n",
    "            iFeature = np.zeros((maxNumAtoms,58))\n",
    "            iFeatureTmp = []\n",
    "            for atom in iMol.GetAtoms():\n",
    "                #atom_result = atom_feature(atom)\n",
    "                iFeatureTmp.append(atom_feature(atom)) #Atom Feature only\n",
    "                #print(atom,\" is \",atom_result)\n",
    "                #iFeatureTmp.append(atom_result) #Atom Feature only\n",
    "                \n",
    "            iFeature[0:len(iFeatureTmp), 0:58] = iFeatureTmp\n",
    "            # Apply 0 padding for feature set.\n",
    "            \n",
    "            features.append(iFeature)\n",
    "            \n",
    "            # Adj- preprocessing\n",
    "            iAdj = np.zeros((maxNumAtoms, maxNumAtoms))\n",
    "            iAdj[0:len(iFeatureTmp), 0:len(iFeatureTmp)] = iAdjTmp + np.eye(len(iFeatureTmp))\n",
    "            adj.append(np.asarray(iAdj))\n",
    "    features = np.asarray(features)\n",
    "    \n",
    "    return features, adj\n",
    "    \n",
    "def atom_feature(atom):\n",
    "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),\n",
    "                                      ['C', 'N', 'O', 'S', 'F', 'H', 'Si', 'P', 'Cl', 'Br',\n",
    "                                       'Li', 'Na', 'K', 'Mg', 'Ca', 'Fe', 'As', 'Al', 'I', 'B',\n",
    "                                       'V', 'Tl', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn',\n",
    "                                       'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'Mn', 'Cr', 'Pt', 'Hg', 'Pb']) +\n",
    "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4]) +\n",
    "                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    [atom.GetIsAromatic()])    # (40, 6, 5, 6, 1)\n",
    "# one_of_k_encoding:\n",
    "# \n",
    "# included value only\n",
    "\n",
    "# one of k encoding unk:\n",
    "# \n",
    "# excluded value is also.\n",
    "\n",
    "def one_of_k_encoding(x,allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise Exception(\"input {0} not in allowable set{1}:\".format(x,allowable_set))\n",
    "    return list(map(lambda s: x==s, allowable_set))\n",
    "\n",
    "def one_of_k_encoding_unk(x,allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        x=allowable_set[-1]\n",
    "    return list(map(lambda s:x==s,allowable_set))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee199330",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-2778d8efe70e>:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook(range(num_mol), desc = 'Reading Data'):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e726d8616e548b1a5f0a119e3988119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading Data:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-2778d8efe70e>:45: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook(smiles_list, desc='Converting to Graph'):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba28622e3e9847a98579cab129eecab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting to Graph:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_smi, list_logP = read_ZINC_smiles('ZINC.smiles',10000)\n",
    "list_feature, list_adj = convert_to_graph(list_smi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4345be99",
   "metadata": {},
   "source": [
    "> Until Here, We could get\n",
    "\n",
    "> - Adjecency Matrix : list_adj\n",
    "> - Node Feature Matrix : list_feature\n",
    "> - logP Value Matrix : list_logP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d8793d",
   "metadata": {},
   "source": [
    "- The Difference of Graph Neural Network is the Calculation that we extract information from the Node Feature Matrix and Adjacency Matrix and predict logP Value.\n",
    "- but In CNN and RNN we did this with only One Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "159012e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_feature[0][0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7144faa8",
   "metadata": {},
   "source": [
    "bb = [10,20,30,40,50,60,70,80,90,100]\n",
    "aa=10\n",
    "cc=list(map(lambda s: aa==s, bb))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "993b6d58",
   "metadata": {},
   "source": [
    "np.array(cc+[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f190b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNDataset(Dataset):\n",
    "    def __init__(self,list_feature,list_adj, list_logP):\n",
    "        self.list_feature = list_feature\n",
    "        self.list_adj = list_adj\n",
    "        self.list_logP = list_logP\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.list_feature)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.list_feature[index], self.list_adj[index], self.list_logP[index]\n",
    "    \n",
    "def partition(list_feature, list_adj, list_logP, args):\n",
    "    num_total = list_feature.shape[0] # length\n",
    "    num_train = int(num_total * (1-args.test_size - args.val_size))\n",
    "    num_val = int(num_total * args.val_size)\n",
    "    num_test = int(num_total*args.test_size)\n",
    "    \n",
    "    ##\n",
    "    feature_train = list_feature[:num_train]\n",
    "    adj_train = list_adj[:num_train]\n",
    "    logP_train = list_logP[:num_train]\n",
    "    \n",
    "    feature_val = list_feature[num_train:num_train+num_val]\n",
    "    adj_val = list_adj[num_train:num_train+num_val]\n",
    "    logP_val = list_logP[num_train:num_train+num_val]\n",
    "    \n",
    "    feature_test = list_feature[:num_train]\n",
    "    adj_test = list_adj[:num_train]\n",
    "    logP_test = list_logP[:num_train]\n",
    "    ##\n",
    "    \n",
    "    train_set = GCNDataset(feature_train, adj_train, logP_train)\n",
    "    val_set = GCNDataset(feature_val, adj_val, logP_val)\n",
    "    test_set = GCNDataset(feature_test, adj_test, logP_test)\n",
    "    \n",
    "    partition={\n",
    "        'train':train_set,\n",
    "        'val':val_set,\n",
    "        'test':test_set\n",
    "    }\n",
    "    \n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2f18902",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_partition = partition(list_feature, list_adj,list_logP,args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c48ca8",
   "metadata": {},
   "source": [
    "# Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70835713",
   "metadata": {},
   "source": [
    "- **GCNLayer** : node feature matrix와 adjacency matrix의 list를 받아 Graph Convolution 연산을 수행하는 Module이다.\n",
    "- (Gate)SkipConnection : ResNet에서 사용되었던 skip connection technique을 구현한 Module이다.\n",
    "- GCNBlock : node feature matrix와 adjacency matrix의 List를받아 원하는 갯수의 GCNLayer를 통과시킨 후, (gated) skip connection을 적용하는 모듈임.\n",
    "- ReadOut : Graph structure에 permutation invariance를 주기 위하여 linear layer를 거친 뒤 batch별로 summation하는 모듈이다.\n",
    "- Predictor : Readout Layer로부터, Graph feature vector로부터 logP value를 예측하기 위한 linear layer module이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69bf58e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, n_atom, act=None, bn=False):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        \n",
    "        self.use_bn = bn\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.bn=nn.BatchNorm1d(n_atom)\n",
    "        self.activation = act\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        out = self.linear(x)\n",
    "        out = torch.matmul(adj, out)\n",
    "        if self.use_bn:\n",
    "            out = self.bn(out)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        return out, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8622f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnection(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(SkipConnection, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        \n",
    "    def forward(self, in_x, out_x):\n",
    "        if(self.in_dim != self.out_dim):\n",
    "            in_x = self.linear(in_x)\n",
    "        out= in_x + out_x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "220ab9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GateSkipConnection(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GateSkipConnection, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        self.linear_coef_in = nn.Linear(out_dim, out_dim)\n",
    "        self.linear_coef_out = nn.Linear(out_dim, out_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, in_x, out_x):\n",
    "        if(self.in_dim != self.out_dim):\n",
    "            in_x = self.linear(in_x)\n",
    "        z = self.gate_coefficient(in_x,out_x)\n",
    "        out = torch.mul(z, out_x) + torch.mul(1.0-z, in_x)\n",
    "        return out\n",
    "    \n",
    "    def gate_coefficient(self, in_x, out_x):\n",
    "        x1 = self.linear_coef_in(in_x)\n",
    "        x2 = self.linear_coef_out(out_x)\n",
    "        return self.sigmoid(x1+x2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d8966d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNBlock(nn.Module):\n",
    "    def __init__(self, n_layer, in_dim, hidden_dim, out_dim,n_atom, bn=True, sc='gsc'):\n",
    "        super(GCNBlock, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(n_layer):\n",
    "            self.layers.append(GCNLayer(in_dim if i==0 else hidden_dim,\n",
    "                                       out_dim if i==n_layer-1 else hidden_dim,\n",
    "                                       n_atom,\n",
    "                                       nn.ReLU() if i!=n_layer-1 else None,\n",
    "                                       bn))\n",
    "        self.relu = nn.ReLU()\n",
    "        if sc=='gsc':\n",
    "            self.sc = GatedSkipConnection(in_dim, out_dim)\n",
    "        elif sc=='sc':\n",
    "            self.sc = SKipConnection(in_dim, out_dim)\n",
    "        elif sc=='no':\n",
    "            self.sc=None\n",
    "        else:\n",
    "            assert False, \"Wrong sc type.\"\n",
    "    def forward(self, x, adj):\n",
    "        residual = x\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            out, adj = layer((x if i==0 else out), adj)\n",
    "        if self.sc != None:\n",
    "            out=self.sc(residual, out)\n",
    "        out = self.relu(out)\n",
    "        return out, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fa5e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadOut(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, act=None):\n",
    "        super(ReadOut, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.linear=nn.Linear(self.in_dim,\n",
    "                             self.out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.activation = act\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = torch.sum(out, 1)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ad4f599",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, act=None):\n",
    "        super(Predictor, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(self.in_dim,\n",
    "                               self.out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.activation=act\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.linear(x)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84df3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNNet(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(GCNNet, self).__init__()\n",
    "        \n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(args.n_block):\n",
    "            self.blocks.append(GCNBlock(args.n_layer,\n",
    "                                       args.in_dim if i==0 else args.hidden_dim,\n",
    "                                       args.hidden_dim,\n",
    "                                       args.hidden_dim,\n",
    "                                       args.n_atom,\n",
    "                                       args.bn,\n",
    "                                       args.sc)\n",
    "                              )\n",
    "        self.readout = ReadOut(args.hidden_dim,\n",
    "                              args.pred_dim1,\n",
    "                              act=nn.ReLU())\n",
    "        self.pred1=Predictor(args.pred_dim1,\n",
    "                            args.pred_dim2,\n",
    "                            act=nn.ReLU())\n",
    "        self.pred1=Predictor(args.pred_dim2,\n",
    "                            args.pred_dim3,\n",
    "                            act=nn.ReLU())\n",
    "        self.pred1=Predictor(args.pred_dim3,\n",
    "                            args.out_dim)\n",
    "    def forward(self, x, adj):\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            out, adj = block((x if i==0 else out), adj)\n",
    "        out = self.readout(out)\n",
    "        out = self.pred1(out)\n",
    "        out = self.pred2(out)\n",
    "        out = self.pred3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943b9b7f",
   "metadata": {},
   "source": [
    "# Train, Validate, Test and Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b7b3fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, partition, optimizer, criterion, args):\n",
    "    trainloader = torch.utils.data.DataLoader(partition['train'],\n",
    "                                             batch_size=args.train_batch_size,\n",
    "                                             shuffle=True, num_workers=2)\n",
    "    net.train()\n",
    "    \n",
    "    train_loss=0.0\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        optimizer.zero_grad() # 매 Epoch마다. zero_grad()가 실행되는 것을 매 iteration마다 실행되도록\n",
    "        \n",
    "        # get the inputs\n",
    "        list_feature, list_adj, list_logP=data\n",
    "        \n",
    "        list_feature = list_feature.cuda().float()\n",
    "        list_adj = list_adj.cuda().float()\n",
    "        list_logP = list_logP.cuda().float().view(-1,1)\n",
    "        outputs = net(list_feature, list_adj)\n",
    "        \n",
    "        loss = criterion(outputs, list_logP)\n",
    "        train_loss +=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step() # for the next step\n",
    "        \n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    return net, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0487632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, partition, criterion, args):\n",
    "    valloader = torch.utils.data.DataLoader(partition['val'],\n",
    "                                           batch_size= args.test_batch_size,\n",
    "                                           shuffle=False, num_workers=2)\n",
    "    net.eval()\n",
    "    val_loss =0\n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            list_feature, list_adj, list_logP = data\n",
    "            \n",
    "            list_feature = list_feature.cuda().float()\n",
    "            list_adj = list_adj.cuda().float()\n",
    "            list_logP = list_logP.cuda().float().view(-1,1)\n",
    "            \n",
    "            outputs = net(list_feature, list_adj)\n",
    "            \n",
    "            loss = criterion(outputs,labels)\n",
    "            val_loss+=loss.item()\n",
    "            \n",
    "        val_loss = val_loss / len(valloader)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "55078e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, partition, args):\n",
    "    testloader = torch.utils.data.DataLoader(partition['test'],\n",
    "                                            batch_size=args.test_batch_size,\n",
    "                                            shuffle=False, num_workers=2)\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        logP_total = list()\n",
    "        pred_logP_total = list()\n",
    "        for data in testloader:\n",
    "            list_feature, list_adj, list_logP = data\n",
    "            \n",
    "            list_feature = list_feature.cuda().float()\n",
    "            list_adj = list_adj.cuda().float()\n",
    "            list_logP = list_logP.cuda().float().view(-1,1)\n",
    "            \n",
    "            logP_total += list_logP.tolist()\n",
    "            list_logP = list_logP.view(-1,1)\n",
    "            \n",
    "            outputs = net(list_feature, list_adj)\n",
    "            pred_logP_total += outputs.view(-1).tolist()\n",
    "            \n",
    "        mae = mean_absolute_error(logP_total,pred_logP_total)\n",
    "        std = np.std(np.array(logP_total)-np.array(pred_logP_total))\n",
    "        \n",
    "    return mae, std, logP_total, pred_logP_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a3a2f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(partition, args):\n",
    "    net = GCNNet()\n",
    "    net.cuda()\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(net.parameters(), lr = args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(net.parameters(), lr = args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice')\n",
    "        \n",
    "    train_losses=[]\n",
    "    val_losses=[]\n",
    "    \n",
    "    for epoch in range(args.epoch):\n",
    "        ts = time.time()\n",
    "        net, train_loss = train(net, partition, optimizer, criterion, args)\n",
    "        val_loss = validate(net, partition, criterion, args)\n",
    "        te = time.time()\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
    "        \n",
    "    mae, std, logP_total, pred_logP_total = test(net, partition, args)\n",
    "    \n",
    "    result = {}\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    result['mae']= mae\n",
    "    result['std']=std\n",
    "    result['logP_total']=logP_total\n",
    "    result['pred_logP_total'] = pred_logP_total\n",
    "    \n",
    "    return vars(args), result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d17fbb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
